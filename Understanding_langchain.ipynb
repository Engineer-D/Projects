{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ldSvzPuyF2Fg",
        "Xhg1d6LbK9LG",
        "-kCesKD-GQwQ",
        "5eIuz9FfeRaN",
        "knNhtz_ueltK",
        "hBJ_5VtbglCP",
        "6tl7GCaXhmmz",
        "OgHenqHYiePn",
        "0Fq2YUZ5jq60",
        "zMX758RskWZL",
        "WTQVwSpLk9BJ",
        "-2C-HZUvlWsZ",
        "Xfw5b_yzmyo1",
        "68KXm0vjnESK",
        "Tnq3R83SoCIc",
        "YUCc8yFIor_R",
        "0OayfTx8qRE_",
        "8Yywhi8IrbvK",
        "pOKbQRyZr-GN",
        "gGOSsmFFsT97",
        "d6jv2rjis5ti",
        "WnKN-9GduPx4"
      ],
      "authorship_tag": "ABX9TyPcpnaknZ9NWPJkmhrrd24/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Engineer-D/Projects/blob/main/Understanding_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### https://github.com/JohannesJolkkonen/funktio-ai-samples/tree/main/knowledge-graph-demo"
      ],
      "metadata": {
        "id": "ldSvzPuyF2Fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDxz_hRgGb7-",
        "outputId": "6c419fe1-465c-4335-b74f-4b174a702198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai (from -r requirements.txt (line 1))\n",
            "  Downloading langchain_openai-0.1.6-py3-none-any.whl (34 kB)\n",
            "Collecting openai (from -r requirements.txt (line 2))\n",
            "  Downloading openai-1.25.1-py3-none-any.whl (312 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.9/312.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting graphdatascience (from -r requirements.txt (line 3))\n",
            "  Downloading graphdatascience-1.10-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting retry==0.9.2 (from -r requirements.txt (line 4))\n",
            "  Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting langchain>=0.0.216 (from -r requirements.txt (line 5))\n",
            "  Downloading langchain-0.1.17-py3-none-any.whl (867 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting streamlit==1.23.1 (from -r requirements.txt (line 6))\n",
            "  Downloading streamlit-1.23.1-py2.py3-none-any.whl (8.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting streamlit-chat==0.0.2.2 (from -r requirements.txt (line 7))\n",
            "  Downloading streamlit_chat-0.0.2.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting streamlit-chat-media==0.0.4 (from -r requirements.txt (line 8))\n",
            "  Downloading streamlit_chat_media-0.0.4-py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (2.0.3)\n",
            "Requirement already satisfied: plotly==5.15.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (5.15.0)\n",
            "Requirement already satisfied: altair<5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (4.2.2)\n",
            "Collecting python-dotenv==1.0.0 (from -r requirements.txt (line 13))\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: decorator>=3.4.2 in /usr/local/lib/python3.10/dist-packages (from retry==0.9.2->-r requirements.txt (line 4)) (4.4.2)\n",
            "Collecting py<2.0.0,>=1.4.26 (from retry==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (8.1.7)\n",
            "Collecting importlib-metadata<7,>=1.4 (from streamlit==1.23.1->-r requirements.txt (line 6))\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Collecting packaging<24,>=14.1 (from streamlit==1.23.1->-r requirements.txt (line 6))\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<10,>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (14.0.2)\n",
            "Collecting pympler<2,>=0.9 (from streamlit==1.23.1->-r requirements.txt (line 6))\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.4 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (8.2.3)\n",
            "Requirement already satisfied: toml<2 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (4.11.0)\n",
            "Collecting tzlocal<5,>=1.1 (from streamlit==1.23.1->-r requirements.txt (line 6))\n",
            "  Downloading tzlocal-4.3.1-py3-none-any.whl (20 kB)\n",
            "Collecting validators<1,>=0.2 (from streamlit==1.23.1->-r requirements.txt (line 6))\n",
            "  Downloading validators-0.28.1-py3-none-any.whl (39 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3 (from streamlit==1.23.1->-r requirements.txt (line 6))\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.1.dev5 (from streamlit==1.23.1->-r requirements.txt (line 6))\n",
            "  Downloading pydeck-0.9.0-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (6.3.3)\n",
            "Collecting watchdog (from streamlit==1.23.1->-r requirements.txt (line 6))\n",
            "  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.46 (from langchain_openai->-r requirements.txt (line 1))\n",
            "  Downloading langchain_core-0.1.50-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken<1,>=0.5.2 (from langchain_openai->-r requirements.txt (line 1))\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai->-r requirements.txt (line 2)) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai->-r requirements.txt (line 2))\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 2)) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 2)) (4.66.2)\n",
            "Collecting multimethod<2.0,>=1.0 (from graphdatascience->-r requirements.txt (line 3))\n",
            "  Downloading multimethod-1.11.2-py3-none-any.whl (10 kB)\n",
            "Collecting neo4j<6.0,>=4.4.2 (from graphdatascience->-r requirements.txt (line 3))\n",
            "  Downloading neo4j-5.20.0.tar.gz (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.0/203.0 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting textdistance<5.0,>=4.0 (from graphdatascience->-r requirements.txt (line 3))\n",
            "  Downloading textdistance-4.6.2-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.216->-r requirements.txt (line 5)) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.216->-r requirements.txt (line 5)) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.216->-r requirements.txt (line 5)) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.216->-r requirements.txt (line 5)) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain>=0.0.216->-r requirements.txt (line 5))\n",
            "  Downloading dataclasses_json-0.6.5-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain>=0.0.216->-r requirements.txt (line 5))\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.36 (from langchain>=0.0.216->-r requirements.txt (line 5))\n",
            "  Downloading langchain_community-0.0.36-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain>=0.0.216->-r requirements.txt (line 5))\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain>=0.0.216->-r requirements.txt (line 5))\n",
            "  Downloading langsmith-0.1.53-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 10)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 10)) (2024.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<5->-r requirements.txt (line 12)) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<5->-r requirements.txt (line 12)) (3.1.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<5->-r requirements.txt (line 12)) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<5->-r requirements.txt (line 12)) (0.12.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216->-r requirements.txt (line 5)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216->-r requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216->-r requirements.txt (line 5)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216->-r requirements.txt (line 5)) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 2)) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 2)) (1.2.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.216->-r requirements.txt (line 5))\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.216->-r requirements.txt (line 5))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3->streamlit==1.23.1->-r requirements.txt (line 6))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 2))\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 2))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit==1.23.1->-r requirements.txt (line 6)) (3.18.1)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain>=0.0.216->-r requirements.txt (line 5))\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<5->-r requirements.txt (line 12)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<5->-r requirements.txt (line 12)) (0.35.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<5->-r requirements.txt (line 12)) (0.18.0)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain>=0.0.216->-r requirements.txt (line 5))\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 2)) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 2)) (2.18.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<5->-r requirements.txt (line 12)) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2->streamlit==1.23.1->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.4->streamlit==1.23.1->-r requirements.txt (line 6)) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.4->streamlit==1.23.1->-r requirements.txt (line 6)) (2.0.7)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.11.0->streamlit==1.23.1->-r requirements.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.11.0->streamlit==1.23.1->-r requirements.txt (line 6)) (2.16.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.0.216->-r requirements.txt (line 5)) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain_openai->-r requirements.txt (line 1)) (2023.12.25)\n",
            "Collecting pytz-deprecation-shim (from tzlocal<5,>=1.1->streamlit==1.23.1->-r requirements.txt (line 6))\n",
            "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3->streamlit==1.23.1->-r requirements.txt (line 6))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.11.0->streamlit==1.23.1->-r requirements.txt (line 6)) (0.1.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.216->-r requirements.txt (line 5))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: neo4j\n",
            "  Building wheel for neo4j (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neo4j: filename=neo4j-5.20.0-py3-none-any.whl size=280771 sha256=d667c07ad4dd6b63b566caaa48055ab32dfaf5e40b6477c7b14f5b729fe462bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/12/66/764554d079caad4b9a11a02cfc0d200dd876d12935b9cf7e64\n",
            "Successfully built neo4j\n",
            "Installing collected packages: watchdog, validators, textdistance, smmap, pytz-deprecation-shim, python-dotenv, pympler, py, packaging, orjson, neo4j, mypy-extensions, multimethod, jsonpointer, importlib-metadata, h11, tzlocal, typing-inspect, tiktoken, retry, pydeck, marshmallow, jsonpatch, httpcore, gitdb, langsmith, httpx, graphdatascience, gitpython, dataclasses-json, openai, langchain-core, streamlit, langchain-text-splitters, langchain_openai, langchain-community, streamlit-chat-media, streamlit-chat, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 7.1.0\n",
            "    Uninstalling importlib_metadata-7.1.0:\n",
            "      Successfully uninstalled importlib_metadata-7.1.0\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 5.2\n",
            "    Uninstalling tzlocal-5.2:\n",
            "      Successfully uninstalled tzlocal-5.2\n",
            "Successfully installed dataclasses-json-0.6.5 gitdb-4.0.11 gitpython-3.1.43 graphdatascience-1.10 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 importlib-metadata-6.11.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.17 langchain-community-0.0.36 langchain-core-0.1.50 langchain-text-splitters-0.0.1 langchain_openai-0.1.6 langsmith-0.1.53 marshmallow-3.21.2 multimethod-1.11.2 mypy-extensions-1.0.0 neo4j-5.20.0 openai-1.25.1 orjson-3.10.3 packaging-23.2 py-1.11.0 pydeck-0.9.0 pympler-1.0.1 python-dotenv-1.0.0 pytz-deprecation-shim-0.1.0.post0 retry-0.9.2 smmap-5.0.1 streamlit-1.23.1 streamlit-chat-0.0.2.2 streamlit-chat-media-0.0.4 textdistance-4.6.2 tiktoken-0.6.0 typing-inspect-0.9.0 tzlocal-4.3.1 validators-0.28.1 watchdog-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BcIiChIXEwRQ"
      },
      "outputs": [],
      "source": [
        "from langchain.graphs import Neo4jGraph\n",
        "from langchain.chains import GraphCypherQAChain\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dotenv\n",
        "import os"
      ],
      "metadata": {
        "id": "pGjA7wfOGlX8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dotenv.load_dotenv()\n",
        "openai_api_key = os.getenv(\"OPEN_AI_SECRET_KEY\")\n",
        "#Neo4j configuration\n",
        "neo4j_url = os.getenv(\"NEO4J_URI\")\n",
        "neo4j_user = os.getenv(\"NEO4J_USERNAME\")\n",
        "neo4j_password = os.getenv(\"NEO4J_PASSWORD\")"
      ],
      "metadata": {
        "id": "lQvBw6znG6zo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",temperature=0, api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "RN2i3mdmHD2n"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cypher generation prompt\n",
        "cypher_generation_template = \"\"\"\n",
        "You are an expert Neo4j Cypher translator who converts English to Cypher based on the Neo4j Schema provided, following the instructions below:\n",
        "1. Generate Cypher query compatible ONLY for Neo4j Version 5\n",
        "2. Do not use EXISTS, SIZE, HAVING keywords in the cypher. Use alias when using the WITH keyword\n",
        "3. Use only Nodes and relationships mentioned in the schema\n",
        "4. Always do a case-insensitive and fuzzy search for any properties related search. Eg: to search for a Client, use `toLower(client.id) contains 'neo4j'`. To search for Slack Messages, use 'toLower(SlackMessage.text) contains 'neo4j'`. To search for a project, use `toLower(project.summary) contains 'logistics platform' OR toLower(project.name) contains 'logistics platform'`.)\n",
        "5. Never use relationships that are not mentioned in the given schema\n",
        "6. When asked about projects, Match the properties using case-insensitive matching and the OR-operator, E.g, to find a logistics platform -project, use `toLower(project.summary) contains 'logistics platform' OR toLower(project.name) contains 'logistics platform'`.\n",
        "\n",
        "schema: {schema}\n",
        "\n",
        "Examples:\n",
        "Question: Which client's projects use most of our people?\n",
        "Answer: ```MATCH (c:CLIENT)<-[:HAS_CLIENT]-(p:Project)-[:HAS_PEOPLE]->(person:Person)\n",
        "RETURN c.name AS Client, COUNT(DISTINCT person) AS NumberOfPeople\n",
        "ORDER BY NumberOfPeople DESC```\n",
        "Question: Which person uses the largest number of different technologies?\n",
        "Answer: ```MATCH (person:Person)-[:USES_TECH]->(tech:Technology)\n",
        "RETURN person.name AS PersonName, COUNT(DISTINCT tech) AS NumberOfTechnologies\n",
        "ORDER BY NumberOfTechnologies DESC```\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "cypher_prompt = PromptTemplate(\n",
        "    template = cypher_generation_template,\n",
        "    input_variables = [\"schema\", \"question\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "O9RXN0EmHbY4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CYPHER_QA_TEMPLATE = \"\"\"You are an assistant that helps to form nice and human understandable answers.\n",
        "The information part contains the provided information that you must use to construct an answer.\n",
        "The provided information is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\n",
        "Make the answer sound as a response to the question. Do not mention that you based the result on the given information.\n",
        "If the provided information is empty, say that you don't know the answer.\n",
        "Final answer should be easily readable and structured.\n",
        "Information:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "qa_prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"], template=CYPHER_QA_TEMPLATE\n",
        ")"
      ],
      "metadata": {
        "id": "qZGldP2-I7CD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_graph(user_input):\n",
        "    graph = Neo4jGraph(url=neo4j_url, username=neo4j_user, password=neo4j_password)\n",
        "    chain = GraphCypherQAChain.from_llm(\n",
        "        llm=llm,\n",
        "        graph=graph,\n",
        "        verbose=True,\n",
        "        return_intermediate_steps=True,\n",
        "        cypher_prompt=cypher_prompt,\n",
        "        qa_prompt=qa_prompt\n",
        "        )\n",
        "    result = chain(user_input)\n",
        "    return result, chain"
      ],
      "metadata": {
        "id": "FA29npfBI6-f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result, chain = query_graph(\"What's the ultimate goal\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY9vy0HVI68T",
        "outputId": "36484325-f4e4-4282-e82f-2aaaebf35d92"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3mMATCH (g:Goal)\n",
            "RETURN g.description AS UltimateGoal\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UP9bwq3I66L",
        "outputId": "6de3f371-d93b-445c-ba7d-d517baac5e71"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': \"What's the ultimate goal\",\n",
              " 'result': \"I don't know the answer.\",\n",
              " 'intermediate_steps': [{'query': 'MATCH (g:Goal)\\nRETURN g.description AS UltimateGoal'},\n",
              "  {'context': []}]}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.qa_chain.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiS7cg9ZJzS-",
        "outputId": "0c7a0644-f18f-478e-f239-8acc75cf4d56"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an assistant that helps to form nice and human understandable answers.\n",
            "The information part contains the provided information that you must use to construct an answer.\n",
            "The provided information is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\n",
            "Make the answer sound as a response to the question. Do not mention that you based the result on the given information.\n",
            "If the provided information is empty, say that you don't know the answer.\n",
            "Final answer should be easily readable and structured.\n",
            "Information:\n",
            "{context}\n",
            "\n",
            "Question: {question}\n",
            "Helpful Answer:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SC test"
      ],
      "metadata": {
        "id": "Xhg1d6LbK9LG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGFmBMRVbxxX",
        "outputId": "7ab97966-7f86-4f3d-e4e8-920313255a0c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "openAIEmbeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "\n",
        "#Are firewall and router configuration standards established and implemented? Please pick the appropriate answer\n",
        "query = \"what is the Firewall Type?\"\n",
        "\n",
        "configurations = [\n",
        "    \"Firewall Configuration:\",\n",
        "    \"-----------------------\",\n",
        "    \"Firewall Type: Cisco ASA\",\n",
        "    \"Security Level: High\",\n",
        "    \"Access Control Lists (ACLs):\",\n",
        "    \"    - ACL 101: Permit TCP any host 192.168.1.1 eq www\",\n",
        "    \"    - ACL 102: Deny IP any any\",\n",
        "    \"\",\n",
        "    \"Router Configuration:\",\n",
        "    \"---------------------\",\n",
        "    \"Router Type: Cisco ISR 4000\",\n",
        "    \"Routing Protocol: OSPF\",\n",
        "    \"Interfaces:\",\n",
        "    \"    - GigabitEthernet0/0: 192.168.1.1/24\",\n",
        "    \"    - GigabitEthernet0/1: 10.0.0.1/24\",\n",
        "    \"    - Serial0/0/0: 172.16.1.1/30\",\n",
        "    \"\",\n",
        "    \"VPN Configuration:\",\n",
        "    \"------------------\",\n",
        "    \"VPN Type: Site-to-Site IPsec VPN\",\n",
        "    \"Tunnel Mode: Tunnel Mode\",\n",
        "    \"Encryption Algorithm: AES-256\",\n",
        "    \"Authentication Method: Pre-shared Key\",\n",
        "    \"Peer IP Address: 203.0.113.2\",\n",
        "    \"Tunnel Interface: Tunnel0\",\n",
        "    \"    - IP Address: 192.168.100.1/30\",\n",
        "    \"    - Peer IP Address: 203.0.113.2\",\n",
        "]\n",
        "\n",
        "# Prompt\n",
        "template = \"\"\"As a compliance assistant you are to help users answer questions presented to them. where possible, reply with just Yes or No\n",
        "example:\n",
        "context: \"Firewall, routers and configuration have been implemented in our network security\"\n",
        "Quesion: Has necessary configuration been put in place?\n",
        "Answer: Yes\n",
        "\n",
        "Instruction: Don't add '.' after Yes, Also keep your answers brief\n",
        "\n",
        "Context: {context}\n",
        "using the context above answer the question {question}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "QA_CHAIN_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=template,\n",
        "    stream = False,\n",
        ")\n",
        "\n",
        "vectorstore = Chroma.from_texts(texts=configurations, embedding=openAIEmbeddings)\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",temperature=0, api_key=openai_api_key)\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectorstore.as_retriever(), #retriever=vectorstore.as_retriever()\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
        ")\n",
        "\n",
        "result = qa_chain({\"query\": query})\n",
        "print(result['result'])"
      ],
      "metadata": {
        "id": "1N0EpoSxJ2G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(qa_chain.combine_documents_chain.llm_chain.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhRPqOWhbsdE",
        "outputId": "636f472d-cf79-4822-c14c-f7916f7aa72c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a compliance assistant you are to help users answer questions presented to them. where possible, reply with just Yes or No\n",
            "example:\n",
            "context: \"Firewall, routers and configuration have been implemented in our network security\"\n",
            "Quesion: Has necessary configuration been put in place?\n",
            "Answer: Yes\n",
            "\n",
            "Instruction: Don't add '.' after Yes, Also keep your answers brief\n",
            "\n",
            "Context: {context}\n",
            "using the context above answer the question {question}\n",
            "Helpful Answer:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain.combine_documents_chain.llm_chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2smf2-HcMlR",
        "outputId": "c174ec30-e0d8-457b-ac89-04d1d9efc57c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template='As a compliance assistant you are to help users answer questions presented to them. where possible, reply with just Yes or No\\nexample:\\ncontext: \"Firewall, routers and configuration have been implemented in our network security\"\\nQuesion: Has necessary configuration been put in place?\\nAnswer: Yes\\n\\nInstruction: Don\\'t add \\'.\\' after Yes, Also keep your answers brief\\n\\nContext: {context}\\nusing the context above answer the question {question}\\nHelpful Answer:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7e33df4a12a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7e33df465ff0>, model_name='gpt-3.5-turbo-0125', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''))"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6S_NaXzwcykd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### https://www.youtube.com/watch?v=J_0qvRt4LNk&list=PL8motc6AQftk1Bs42EW45kwYbyJ4jOdiZ"
      ],
      "metadata": {
        "id": "-kCesKD-GQwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install openai langchain langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vr9G1X-DGQcX",
        "outputId": "0e7671be-13b8-4873-8cbc-3c784cfd5230"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/1.8 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('OPENAI_KEY')"
      ],
      "metadata": {
        "id": "jcg7n1uQGel2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "63sjoO8AGQWw"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo',\n",
        "             temperature=0.9,\n",
        "             max_tokens = 256,\n",
        "            api_key = api_key)"
      ],
      "metadata": {
        "id": "_fLAV6dHGQUp"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate"
      ],
      "metadata": {
        "id": "OLJeCzwfGQSU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restaurant_template = \"\"\"\n",
        "I want you to act as a naming consultant for new restaurants.\n",
        "\n",
        "Return a list of restaurant names. Each name should be short, catchy and easy to remember. It shoud relate to the type of restaurant you are naming.\n",
        "\n",
        "What are some good names for a restaurant that is {restaurant_desription}?\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"restaurant_desription\"],\n",
        "    template=restaurant_template,\n",
        ")"
      ],
      "metadata": {
        "id": "RgHE4j4fHBHs"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "description = \"a Greek place that serves fresh lamb souvlakis and other Greek food \"\n",
        "description_02 = \"a burger place that is themed with baseball memorabilia\"\n",
        "description_03 = \"a cafe that has live hard rock music and memorabilia\"\n",
        "\n",
        "## to see what the prompt will be like\n",
        "print(prompt_template.format(restaurant_desription=description))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhk7pUpxHHGC",
        "outputId": "74632e3f-abb6-4cb6-9ff4-49be48c5eaca"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "I want you to act as a naming consultant for new restaurants.\n",
            "\n",
            "Return a list of restaurant names. Each name should be short, catchy and easy to remember. It shoud relate to the type of restaurant you are naming.\n",
            "\n",
            "What are some good names for a restaurant that is a Greek place that serves fresh lamb souvlakis and other Greek food ?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## querying the model with the prompt template\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "\n",
        "chain = LLMChain(llm=llm,\n",
        "                 prompt=prompt_template)\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "print(chain.run(description))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W_gYokrHLcT",
        "outputId": "d96dfb0b-29fa-4647-e1c0-22b2410695c5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Lamb & Olive\n",
            "2. Greek Grill House\n",
            "3. Souvlaki Spot\n",
            "4. Gyro Haven\n",
            "5. Acropolis Eats\n",
            "6. Zeus' Bites\n",
            "7. Olive Tree Kitchen\n",
            "8. Mykonos Grille\n",
            "9. The Greek Table\n",
            "10. Hellenic Flavors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the chain only specifying the input variable.\n",
        "print(chain.run({\"restaurant_desription\":description_02}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlZ2oTsSHWW-",
        "outputId": "59470bf1-287f-4cf1-ec88-b86af78db7c5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Home Run Burgers\n",
            "2. Grand Slam Grill\n",
            "3. The Ballpark Burger Co.\n",
            "4. Bat & Burger Bistro\n",
            "5. Diamond Diner\n",
            "6. Burger Base\n",
            "7. Triple Play Burgers\n",
            "8. Slugger's Burgers & Fries\n",
            "9. The All-Star Burger Joint\n",
            "10. Batter Up Bites\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ihfWFCmxIpOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### https://github.com/gkamradt/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%201%20-%20Fundamentals.ipynb"
      ],
      "metadata": {
        "id": "5eIuz9FfeRaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### LangChain Cookbook 👨‍🍳👩‍🍳\n",
        "\n",
        "*This cookbook is based off the [LangChain Conceptual Documentation](https://docs.langchain.com/docs/)*\n",
        "\n",
        "**Goal:** Provide an introductory understanding of the components and use cases of LangChain via [ELI5](https://www.dictionary.com/e/slang/eli5/#:~:text=ELI5%20is%20short%20for%20%E2%80%9CExplain,a%20complicated%20question%20or%20problem.) examples and code snippets. For use cases check out [part 2](https://github.com/gkamradt/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%202%20-%20Use%20Cases.ipynb). See [video tutorial](https://www.youtube.com/watch?v=2xxziIWmaSA) of this notebook.\n",
        "\n",
        " **Links:**  \n",
        "* [LC Conceptual Documentation](https://docs.langchain.com/docs/)\n",
        "* [LC Python Documentation](https://python.langchain.com/en/latest/)\n",
        "* [LC Javascript/Typescript Documentation](https://js.langchain.com/docs/)\n",
        "* [LC Discord](https://discord.gg/6adMQxSpJS)\n",
        "* [www.langchain.com](https://langchain.com/)\n",
        "* [LC Twitter](https://twitter.com/LangChainAI)  \n",
        "\n",
        "### **What is LangChain?**\"\n",
        "> LangChain is a framework for developing applications powered by language models.\n",
        "    \n",
        "**~~TL~~DR**: LangChain makes the complicated parts of working & building with AI models easier. It helps do this in two ways:\n",
        "\n",
        "1. **Integration** - Bring external data, such as your files, other applications, and api data, to your LLMs\\n\",\n",
        "2. **Agency** - Allow your LLMs to interact with it's environment via decision making. Use LLMs to help decide which action to take next  \n",
        "\n",
        "### **Why LangChain?**\n",
        "1. **Components** - LangChain makes it easy to swap out abstractions and components necessary to work with language models.\n",
        "2. **Customized Chains** - LangChain provides out of the box support for using and customizing 'chains' - a series of actions strung together.\n",
        "3. **Speed 🚢** - This team ships insanely fast. You'll be up to date with the latest LLM features.\n",
        "4. **Community 👥** - Wonderful discord and community support, meet ups, hackathons, etc.\n",
        "Though LLMs can be straightforward (text-in, text-out) you'll quickly run into friction points that LangChain helps with once you develop more complicated applications.\n",
        "\n",
        "*Note: This cookbook will not cover all aspects of LangChain. It's contents have been curated to get you to building & impact as quick as possible. For more, please check out [LangChain Conceptual Documentation](https://docs.langchain.com/docs/)*\n",
        "\n",
        "*Update Oct '23: This notebook has been expanded from it's original form*\n",
        "\n",
        "You'll need an OpenAI api key to follow this tutorial. You can have it as an environement variable, in an .env file where this jupyter notebook lives, or insert it below where 'YourAPIKey' is. Have if you have questions on this, put these instructions into [ChatGPT](https://chat.openai.com/)."
      ],
      "metadata": {
        "id": "knNhtz_ueltK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langchain langchain_openai chromadb unstructured"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA49-fYVeV7E",
        "outputId": "3a244129-981e-4a6e-d9e3-12d4c0876e8c"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('OPENAI_KEY')"
      ],
      "metadata": {
        "id": "EHKS_Xp-gefZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### LangChain Components  \n",
        "\n",
        "Schema - Nuts and Bolts of working with Large Language Models (LLMs)  \n",
        "\n",
        "Text  \n",
        "The natural language way to interact with LLMs"
      ],
      "metadata": {
        "id": "hBJ_5VtbglCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You'll be working with simple strings (that'll soon grow in complexity!)\n",
        "my_text = \"What day comes after Friday?\"\n",
        "my_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Wn1a8wUWeV3q",
        "outputId": "6fcfd226-fccf-412a-e634-cf1050990a40"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What day comes after Friday?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Chat Messages\n",
        "Like text, but specified with a message type (System, Human, AI)\n",
        "\n",
        "* System - Helpful background context that tell the AI what to do\n",
        "* Human - Messages that are intented to represent the user\n",
        "* AI - Messages that show what the AI responded with  \n",
        "\n",
        "For more, see OpenAI's [documentation](https://platform.openai.com/docs/guides/chat/introduction)"
      ],
      "metadata": {
        "id": "6tl7GCaXhmmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "# This it the language model we'll use. We'll talk about what we're doing below in the next section\n",
        "chat = ChatOpenAI(temperature=.7, openai_api_key=api_key)"
      ],
      "metadata": {
        "id": "_cnw_CLEeV1D"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create a few messages that simulate a chat experience with a bot"
      ],
      "metadata": {
        "id": "dp0Ga88ph-2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out what to eat in one short sentence\"),\n",
        "        HumanMessage(content=\"I like tomatoes, what should I eat?\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrQDQGNmeVyY",
        "outputId": "8009dd87-2d00-4978-e7a8-12d4e232f16e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='You might enjoy a tomato, basil, and mozzarella salad.', response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 39, 'total_tokens': 52}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_a450710239', 'finish_reason': 'stop', 'logprobs': None}, id='run-f1d21361-630f-4aa2-af52-52348183a8ec-0')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also pass more chat history w/ responses from the AI"
      ],
      "metadata": {
        "id": "vcVlDhAyiWMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out where to travel in one short sentence\"),\n",
        "        HumanMessage(content=\"I like the beaches where should I go?\"),\n",
        "        AIMessage(content=\"You should go to Nice, France\"),\n",
        "        HumanMessage(content=\"What else should I do when I'm there?\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c42w9kreVv9",
        "outputId": "02911220-4a2f-46cb-cd0d-f769e7be1a9c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Explore the charming Old Town and enjoy the vibrant street markets in Nice, France.', response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 64, 'total_tokens': 80}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_a450710239', 'finish_reason': 'stop', 'logprobs': None}, id='run-662a715a-4b7a-4ffc-ba00-cf6e3d285b53-0')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also exclude the system message if you want"
      ],
      "metadata": {
        "id": "xGsupzHcib8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\n",
        "    [\n",
        "        HumanMessage(content=\"What day comes after Thursday?\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hyND1DJeVtp",
        "outputId": "414b030e-8775-45b2-9ff9-30c657f00131"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Friday', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 13, 'total_tokens': 14}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None}, id='run-6c876534-e6b2-46ad-923f-5da4893343b7-0')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Documents**  \n",
        "An object that holds a piece of text and metadata (more information about that text)"
      ],
      "metadata": {
        "id": "OgHenqHYiePn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document"
      ],
      "metadata": {
        "id": "KGxylaS1eVrO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\",\n",
        "         metadata={\n",
        "             'my_document_id' : 234234,\n",
        "             'my_document_source' : \"The LangChain Papers\",\n",
        "             'my_document_create_time' : 1680013019\n",
        "         })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZkD_FngeVpC",
        "outputId": "9b00a849-d287-4ee5-9b7b-7838af1933b0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\", metadata={'my_document_id': 234234, 'my_document_source': 'The LangChain Papers', 'my_document_create_time': 1680013019})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But you don't have to include metadata if you don't want to"
      ],
      "metadata": {
        "id": "1HoXTJu3jIEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfpEHAiAeVmy",
        "outputId": "3a6ea12c-bb34-4557-94c2-1561576c58f2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Models - The interface to the AI brains\n",
        "**Chat Model**  \n",
        "A model that takes a series of messages and returns a message output"
      ],
      "metadata": {
        "id": "0Fq2YUZ5jq60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(temperature=1, openai_api_key=api_key)"
      ],
      "metadata": {
        "id": "Mrj33a5IjLRp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\n",
        "    [\n",
        "        SystemMessage(content=\"You are an unhelpful AI bot that makes a joke at whatever the user says\"),\n",
        "        HumanMessage(content=\"I would like to go to New York, how should I do this?\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jV3-PsMkPch",
        "outputId": "fc4f0384-6f4d-4775-c0dc-c481402df6cc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Just take a giant slingshot and aim for the East Coast!', response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 43, 'total_tokens': 57}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None}, id='run-8cec0001-2970-4b47-9f77-089a6a6d0a54-0')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Function Calling Models\n",
        "[Function calling models](https://openai.com/blog/function-calling-and-other-api-updates) are similar to Chat Models but with a little extra flavor. They are fine tuned to give structured data outputs.  \n",
        "\n",
        "This comes in handy when you're making an API call to an external service or doing extraction."
      ],
      "metadata": {
        "id": "zMX758RskWZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(model='gpt-3.5-turbo-0613', temperature=1, openai_api_key=api_key)\n",
        "\n",
        "output = chat(messages=\n",
        "     [\n",
        "         SystemMessage(content=\"You are an helpful AI bot\"),\n",
        "         HumanMessage(content=\"What’s the weather like in Boston right now?\")\n",
        "     ],\n",
        "     functions=[{\n",
        "         \"name\": \"get_current_weather\",\n",
        "         \"description\": \"Get the current weather in a given location\",\n",
        "         \"parameters\": {\n",
        "             \"type\": \"object\",\n",
        "             \"properties\": {\n",
        "                 \"location\": {\n",
        "                     \"type\": \"string\",\n",
        "                     \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
        "                 },\n",
        "                 \"unit\": {\n",
        "                     \"type\": \"string\",\n",
        "                     \"enum\": [\"celsius\", \"fahrenheit\"]\n",
        "                 }\n",
        "             },\n",
        "             \"required\": [\"location\"]\n",
        "         }\n",
        "     }\n",
        "     ]\n",
        ")\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECNXb7M-kTGF",
        "outputId": "f1bc0e8a-0e03-4b3e-b1ec-3c5ba8e1c0f5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"location\": \"Boston, MA\"\\n}', 'name': 'get_current_weather'}}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 91, 'total_tokens': 109}, 'model_name': 'gpt-3.5-turbo-0613', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-a9b59cd2-4a59-4cbe-b8b0-d338b79474c7-0')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See the extra additional_kwargs that is passed back to us? We can take that and pass it to an external API to get data. It saves the hassle of doing output parsing."
      ],
      "metadata": {
        "id": "qTje9yhZk61g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Text Embedding Model\n",
        "Change your text into a vector (a series of numbers that hold the semantic 'meaning' of your text). Mainly used when comparing two pieces of text together.\n",
        "\n",
        "*BTW: Semantic means 'relating to meaning in language or logic.*"
      ],
      "metadata": {
        "id": "WTQVwSpLk9BJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=api_key)"
      ],
      "metadata": {
        "id": "LS67MNbCkqpY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hi! It's time for the beach\""
      ],
      "metadata": {
        "id": "bHqpEuWWlPyy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_embedding = embeddings.embed_query(text)\n",
        "print (f\"Here's a sample: {text_embedding[:5]}...\")\n",
        "print (f\"Your embedding is length {len(text_embedding)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yLlBQ8flRnc",
        "outputId": "3f08af18-35b0-47ce-8403-4e941771defb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a sample: [-0.0001879176858456097, -0.0030974280186882763, -0.0010647408232164338, -0.01923793187847746, -0.015148358467339893]...\n",
            "Your embedding is length 1536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Prompts - Text generally used as instructions to your model  \n",
        "**Prompt**  \n",
        "What you'll pass to the underlying model"
      ],
      "metadata": {
        "id": "-2C-HZUvlWsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(temperature=1, openai_api_key=api_key)\n",
        "\n",
        "# I like to use three double quotation marks for my prompts because it's easier to read\n",
        "prompt = \"\"\"\n",
        "Today is Monday, tomorrow is Wednesday.\n",
        "\n",
        "What is wrong with that statement?\n",
        "\"\"\"\n",
        "\n",
        "chat.invoke(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPRm7S9KlUcC",
        "outputId": "3867e76f-2a4f-4420-e620-b1b444956eea"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The statement is incorrect. Tomorrow is Tuesday, not Wednesday.', response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 23, 'total_tokens': 35}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None}, id='run-7f7252a6-11c8-447c-a2c4-763dc4222bdc-0')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Prompt Template\n",
        "An object that helps create prompts based on a combination of user input, other non-static information and a fixed template string.\n",
        "\n",
        "Think of it as an f-string in python but for prompts\n",
        "\n",
        "Advanced: Check out LangSmithHub(https://smith.langchain.com/hub) for many more communit prompt templates"
      ],
      "metadata": {
        "id": "Xfw5b_yzmyo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# Notice \"location\" below, that is a placeholder for another value later\n",
        "template = \"\"\"\n",
        "I really want to travel to {location}. What should I do there?\n",
        "\n",
        "Respond in one short sentence\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"location\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "final_prompt = prompt.format(location='Rome')\n",
        "\n",
        "print (f\"Final Prompt: {final_prompt}\")\n",
        "print (\"-----------\")\n",
        "print (f\"LLM Output: {chat.invoke(final_prompt)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NdZDq6JmGT1",
        "outputId": "18794df4-168d-459d-c0b6-aa5e15ec446e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Prompt: \n",
            "I really want to travel to Rome. What should I do there?\n",
            "\n",
            "Respond in one short sentence\n",
            "\n",
            "-----------\n",
            "LLM Output: content='Visit iconic landmarks such as the Colosseum, Vatican City, and Trevi Fountain.' response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 28, 'total_tokens': 47}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_a450710239', 'finish_reason': 'stop', 'logprobs': None} id='run-4bfaca80-1c77-4b66-85d0-cf3238f99d91-0'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Example Selectors\n",
        "An easy way to select from a series of examples that allow you to dynamic place in-context information into your prompt. Often used when your task is nuanced or you have a large list of examples.\n",
        "\n",
        "Check out different types of example selectors [here](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/)\n",
        "\n",
        "If you want an overview on why examples are important (prompt engineering), check out [this video](https://www.youtube.com/watch?v=dOxUroR57xs)"
      ],
      "metadata": {
        "id": "68KXm0vjnESK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
        ")\n",
        "\n",
        "# Examples of locations that nouns are found\n",
        "examples = [\n",
        "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
        "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
        "    {\"input\": \"driver\", \"output\": \"car\"},\n",
        "    {\"input\": \"tree\", \"output\": \"ground\"},\n",
        "    {\"input\": \"bird\", \"output\": \"nest\"},\n",
        "]"
      ],
      "metadata": {
        "id": "OMk50nl6m-XW"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SemanticSimilarityExampleSelector will select examples that are similar to your input by semantic meaning\n",
        "\n",
        "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    # This is the list of examples available to select from.\n",
        "    examples,\n",
        "\n",
        "    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.\n",
        "    OpenAIEmbeddings(openai_api_key=api_key),\n",
        "\n",
        "    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.\n",
        "    Chroma,\n",
        "\n",
        "    # This is the number of examples to produce.\n",
        "    k=2\n",
        ")"
      ],
      "metadata": {
        "id": "Wzjzueg6nXD6"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similar_prompt = FewShotPromptTemplate(\n",
        "    # The object that will help select examples\n",
        "    example_selector=example_selector,\n",
        "\n",
        "    # Your prompt\n",
        "    example_prompt=example_prompt,\n",
        "\n",
        "    # Customizations that will be added to the top and bottom of your prompt\n",
        "    prefix=\"Give the location an item is usually found in\",\n",
        "    suffix=\"Input: {noun}\\nOutput:\",\n",
        "\n",
        "    # What inputs your prompt will receive\n",
        "    input_variables=[\"noun\"],\n",
        ")"
      ],
      "metadata": {
        "id": "ZrbjRwqqnkZL"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a noun!\n",
        "my_noun = \"plant\"\n",
        "# my_noun = \"student\"\n",
        "\n",
        "print(similar_prompt.format(noun=my_noun))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4RH885an3eo",
        "outputId": "9264871b-2b1c-4ba1-9444-b946909c98a4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the location an item is usually found in\n",
            "\n",
            "Example Input: tree\n",
            "Example Output: ground\n",
            "\n",
            "Example Input: bird\n",
            "Example Output: nest\n",
            "\n",
            "Input: plant\n",
            "Output:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat.invoke(similar_prompt.format(noun=my_noun))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVPWjZlgn5qK",
        "outputId": "4607beea-1f27-4a1b-bcc5-73784fe53bf7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='soil', response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 43, 'total_tokens': 45}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_a450710239', 'finish_reason': 'stop', 'logprobs': None}, id='run-4ae8703d-15a7-4a8f-b579-de67a55117a9-0')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Output Parsers Method 1: Prompt Instructions & String Parsing\n",
        "A helpful way to format the output of a model. Usually used for structured output. LangChain has a bunch more output parsers listed on their documentation.\n",
        "\n",
        "Two big concepts:\n",
        "\n",
        "1. Format Instructions - A autogenerated prompt that tells the LLM how to format it's response based off your desired result\n",
        "\n",
        "2. Parser - A method which will extract your model's text output into a desired structure (usually json)"
      ],
      "metadata": {
        "id": "Tnq3R83SoCIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate"
      ],
      "metadata": {
        "id": "w4UUX_ktn-5y"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How you would like your response structured. This is basically a fancy prompt template\n",
        "response_schemas = [\n",
        "    ResponseSchema(name=\"bad_string\", description=\"This a poorly formatted user input string\"),\n",
        "    ResponseSchema(name=\"good_string\", description=\"This is your response, a reformatted response\")\n",
        "]\n",
        "\n",
        "# How you would like to parse your output\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ],
      "metadata": {
        "id": "dLCKWCdpoMNj"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See the prompt template you created for formatting\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "print (format_instructions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1NZGEMaoXun",
        "outputId": "07e3f311-4586-48bb-aba3-d1b7b80a340b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"bad_string\": string  // This a poorly formatted user input string\n",
            "\t\"good_string\": string  // This is your response, a reformatted response\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "You will be given a poorly formatted string from a user.\n",
        "Reformat it and make sure all the words are spelled correctly\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "% USER INPUT:\n",
        "{user_input}\n",
        "\n",
        "YOUR RESPONSE:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"user_input\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions},\n",
        "    template=template\n",
        ")\n",
        "\n",
        "promptValue = prompt.format(user_input=\"welcom to califonya!\")\n",
        "\n",
        "print(promptValue)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHs0oql4odvs",
        "outputId": "e27fa03e-1572-492d-a5a8-03f1565e6f84"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You will be given a poorly formatted string from a user.\n",
            "Reformat it and make sure all the words are spelled correctly\n",
            "\n",
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"bad_string\": string  // This a poorly formatted user input string\n",
            "\t\"good_string\": string  // This is your response, a reformatted response\n",
            "}\n",
            "```\n",
            "\n",
            "% USER INPUT:\n",
            "welcom to califonya!\n",
            "\n",
            "YOUR RESPONSE:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_output = chat.invoke(promptValue)\n",
        "llm_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyNawixPog0I",
        "outputId": "946a0795-8f7c-4b9a-8e44-756ab9f1a75b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='```json\\n{\\n\\t\"bad_string\": \"welcom to califonya!\",\\n\\t\"good_string\": \"welcome to california!\"\\n}\\n```', response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 116, 'total_tokens': 146}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None}, id='run-67ba7708-e359-4b5e-80f6-f4f6e629c6da-0')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Output Parsers Method 2: OpenAI Fuctions\n",
        "When OpenAI released function calling, the game changed. This is recommended method when starting out.\n",
        "\n",
        "They trained models specifically for outputing structured data. It became super easy to specify a Pydantic schema and get a structured output.\n",
        "\n",
        "There are many ways to define your schema, I prefer using Pydantic Models because of how organized they are. Feel free to reference OpenAI's [documention](https://platform.openai.com/docs/guides/gpt/function-calling) for other methods.\n",
        "\n",
        "In order to use this method you'll need to use a model that supports [function calling](https://openai.com/blog/function-calling-and-other-api-updates#:~:text=Developers%20can%20now%20describe%20functions%20to%20gpt%2D4%2D0613%20and%20gpt%2D3.5%2Dturbo%2D0613%2C). I'll use gpt4-0613\n",
        "\n",
        "Example 1: Simple\n",
        "\n",
        "Let's get started by defining a simple model for us to extract from."
      ],
      "metadata": {
        "id": "YUCc8yFIor_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from typing import Optional\n",
        "\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Identifying information about a person.\"\"\"\n",
        "\n",
        "    name: str = Field(..., description=\"The person's name\")\n",
        "    age: int = Field(..., description=\"The person's age\")\n",
        "    fav_food: Optional[str] = Field(None, description=\"The person's favorite food\")"
      ],
      "metadata": {
        "id": "wEmpTt1SooT7"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then let's create a chain (more on this later) that will do the extracting for us"
      ],
      "metadata": {
        "id": "6S2p9s2ppOr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.openai_functions import create_structured_output_chain\n",
        "\n",
        "llm = ChatOpenAI(model='gpt-4-0613', openai_api_key=api_key)\n",
        "\n",
        "chain = create_structured_output_chain(Person, llm, prompt)\n",
        "chain.run(\n",
        "    \"Sally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally.\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkUdaB48pKgp",
        "outputId": "bb6f403b-7de5-4c25-a9c0-11d03c3c1353"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Person(name='Sally', age=13, fav_food='spinach')"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how we only have data on one person from that list? That is because we didn't specify we wanted multiple. Let's change our schema to specify that we want a list of people if possible."
      ],
      "metadata": {
        "id": "KTEmGPYHpgpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Sequence\n",
        "\n",
        "class People(BaseModel):\n",
        "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
        "\n",
        "    people: Sequence[Person] = Field(..., description=\"The people in the text\")"
      ],
      "metadata": {
        "id": "3fNUy0ezpTqS"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll call for People rather than Person"
      ],
      "metadata": {
        "id": "wYxcok5Mp6nP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = create_structured_output_chain(People, llm, prompt)\n",
        "chain.run(\n",
        "    \"Sally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally.\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oyw07zZUp28m",
        "outputId": "98e4c258-15fc-4b22-deef-3bac3444419e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "People(people=[Person(name='Sally', age=13, fav_food=''), Person(name='Joey', age=12, fav_food='spinach'), Person(name='Caroline', age=23, fav_food='')])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do some more parsing with it\n",
        "\n",
        "Example 2: Enum\n",
        "\n",
        "Now let's parse when a product from a list is mentioned"
      ],
      "metadata": {
        "id": "GekF3NpFqCZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "\n",
        "class Product(str, enum.Enum):\n",
        "    CRM = \"CRM\"\n",
        "    VIDEO_EDITING = \"VIDEO_EDITING\"\n",
        "    HARDWARE = \"HARDWARE\""
      ],
      "metadata": {
        "id": "41yGZWkwp9Z9"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Products(BaseModel):\n",
        "    \"\"\"Identifying products that were mentioned in a text\"\"\"\n",
        "\n",
        "    products: Sequence[Product] = Field(..., description=\"The products mentioned in a text\")"
      ],
      "metadata": {
        "id": "SDLeaIeQqGDH"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = create_structured_output_chain(Products, llm, prompt)\n",
        "chain.run(\n",
        "    \"The CRM in this demo is great. Love the hardware. The microphone is also cool. Love the video editing\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HihZnjXJqGAK",
        "outputId": "d4f317d5-ae25-4757-a628-3a8328d95e03"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Products(products=[<Product.CRM: 'CRM'>, <Product.HARDWARE: 'HARDWARE'>, <Product.VIDEO_EDITING: 'VIDEO_EDITING'>])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Indexes - Structuring documents to LLMs can work with them\n",
        "**Document Loaders**  \n",
        "Easy ways to import data from other sources. Shared functionality with [OpenAI Plugins specifically retrieval plugins](https://github.com/openai/chatgpt-retrieval-plugin)\n",
        "\n",
        "See a [big list](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) of document loaders here. A bunch more on [Llama Index](https://llamahub.ai/) as well.  \n",
        "\n",
        "HackerNews"
      ],
      "metadata": {
        "id": "0OayfTx8qRE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import HNLoader"
      ],
      "metadata": {
        "id": "_MwZ5FI7qF9u"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")"
      ],
      "metadata": {
        "id": "Xp9aKatSqF7E"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()"
      ],
      "metadata": {
        "id": "8F88FUx2qFJO"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f\"Found {len(data)} comments\")\n",
        "print (f\"Here's a sample:\\n\\n{''.join([x.page_content[:150] for x in data[:2]])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3MTe1OIqrqm",
        "outputId": "85d24ce4-683e-476f-da20-c28d030c9f8d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 76 comments\n",
            "Here's a sample:\n",
            "\n",
            "Ozzie_osman on Jan 18, 2023  \n",
            "             | next [–] \n",
            "\n",
            "LangChain is awesome. For people not sure what it's doing, large language models (LLMs) are veOzzie_osman on Jan 18, 2023  \n",
            "             | parent | next [–] \n",
            "\n",
            "Also, another library to check out is GPT Index (https://github.com/jerryjliu/gpt_ind\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Books from Gutenberg Project"
      ],
      "metadata": {
        "id": "UmwNQyeOqwbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import GutenbergLoader\n",
        "\n",
        "loader = GutenbergLoader(\"https://www.gutenberg.org/cache/epub/2148/pg2148.txt\")\n",
        "\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "aEmsUIsMquKx"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[0].page_content[1855:1984])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJrzB9jGqy9I",
        "outputId": "c210786f-91be-4f81-ae72-9e28188d5ee4"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " nimio.—_Seneca_.\r\n",
            "\n",
            "\n",
            "\r\n",
            "\n",
            "\n",
            "      At Paris, just after dark one gusty evening in the autumn of 18-,\r\n",
            "\n",
            "\n",
            "      I was enjoying the twof\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "URLs and webpages\n",
        "\n",
        "Let's try it out with [Paul Graham's website](http://www.paulgraham.com/)"
      ],
      "metadata": {
        "id": "sEhZU_5CrCVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "\n",
        "urls = [\n",
        "    \"http://www.paulgraham.com/\",\n",
        "]\n",
        "\n",
        "loader = UnstructuredURLLoader(urls=urls)\n",
        "\n",
        "data = loader.load()\n",
        "\n",
        "data[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "wm5FOsknqzus",
        "outputId": "b7901ee2-c0f8-4ef6-ba29-f576bb400fbd"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'New: How to Start Google | Best Essay | Superlinear Want to start a startup? Get funded by Y Combinator . © mmxxiv pg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Text Splitters\n",
        "Often times your document is too long (like a book) for your LLM. You need to split it up into chunks. Text splitters help with this.\n",
        "\n",
        "There are many ways you could split your text into chunks, experiment with [different ones](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html) to see which is best for you."
      ],
      "metadata": {
        "id": "8Yywhi8IrbvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "-LyPUcEpqzsk"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a long document we can split up.\n",
        "with open('data/PaulGrahamEssays/worked.txt') as f:\n",
        "    pg_work = f.read()\n",
        "\n",
        "print (f\"You have {len([pg_work])} document\")"
      ],
      "metadata": {
        "id": "CeIlZuNZqzql"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 150,\n",
        "    chunk_overlap  = 20,\n",
        ")\n",
        "\n",
        "texts = text_splitter.create_documents([pg_work])"
      ],
      "metadata": {
        "id": "xsX6RMbNqzoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f\"You have {len(texts)} documents\")"
      ],
      "metadata": {
        "id": "Gzw6WV2Iqzma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Preview:\")\n",
        "print (texts[0].page_content, \"\\n\")\n",
        "print (texts[1].page_content)"
      ],
      "metadata": {
        "id": "eu71r_FKqzgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Retrievers\n",
        "Easy way to combine documents with language models.\n",
        "\n",
        "There are many different types of retrievers, the most widely supported is the VectoreStoreRetriever"
      ],
      "metadata": {
        "id": "pOKbQRyZr-GN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "6EoaksdLsCDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get your splitter ready\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "\n",
        "# Split your docs into texts\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Get embedding engine ready\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
        "\n",
        "# Embedd your texts\n",
        "db = FAISS.from_documents(texts, embeddings)"
      ],
      "metadata": {
        "id": "haQCf6gqsG7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Init your retriever. Asking for just 1 document back\n",
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "YbXUgyIVsG4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever"
      ],
      "metadata": {
        "id": "nw7uXmU8sG1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = retriever.get_relevant_documents(\"what types of things did the author want to build?\")"
      ],
      "metadata": {
        "id": "RatTTAqQsLMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n\".join([x.page_content[:200] for x in docs[:2]]))"
      ],
      "metadata": {
        "id": "DiOQ0SlLsLJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### VectorStores\n",
        "Databases to store vectors. Most popular ones are Pinecone & Weaviate. More examples on OpenAIs retriever documentation. Chroma & FAISS are easy to work with locally.\n",
        "\n",
        "Conceptually, think of them as tables w/ a column for embeddings (vectors) and a column for metadata.\n",
        "\n",
        "Example  \n",
        "\n",
        "Embedding________________________________________________Metadata  \n",
        "[-0.00015641732898075134, -0.003165106289088726, ...]\t{'date' : '1/2/23}  \n",
        "[-0.00035465431654651654, 1.4654131651654516546, ...]\t{'date' : '1/3/23}  "
      ],
      "metadata": {
        "id": "gGOSsmFFsT97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
        "documents = loader.load()\n",
        "\n",
        "# Get your splitter ready\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "\n",
        "# Split your docs into texts\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Get embedding engine ready\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=api_key)"
      ],
      "metadata": {
        "id": "IcjpPo6KsLHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f\"You have {len(texts)} documents\")"
      ],
      "metadata": {
        "id": "ZGqwFBT_sLE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_list = embeddings.embed_documents([text.page_content for text in texts])"
      ],
      "metadata": {
        "id": "kA4-I5gssLCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f\"You have {len(embedding_list)} embeddings\")\n",
        "print (f\"Here's a sample of one: {embedding_list[0][:3]}...\")"
      ],
      "metadata": {
        "id": "IHnHBMF7s07S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your vectorstore store your embeddings (☝️) and make them easily searchable"
      ],
      "metadata": {
        "id": "4oV6f4bFs37_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Memory  \n",
        "Helping LLMs remember information.\n",
        "\n",
        "Memory is a bit of a loose term. It could be as simple as remembering information you've chatted about in the past or more complicated information retrieval.\n",
        "\n",
        "We'll keep it towards the Chat Message use case. This would be used for chat bots.\n",
        "\n",
        "There are many types of memory, explore [the documentation](https://python.langchain.com/en/latest/modules/memory/how_to_guides.html) to see which one fits your use case.  \n",
        "\n",
        "**Chat Message History**"
      ],
      "metadata": {
        "id": "d6jv2rjis5ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ChatMessageHistory\n",
        "\n",
        "chat = ChatOpenAI(temperature=0, openai_api_key=api_key)\n",
        "\n",
        "history = ChatMessageHistory()\n",
        "\n",
        "history.add_ai_message(\"hi!\")\n",
        "\n",
        "history.add_user_message(\"what is the capital of france?\")"
      ],
      "metadata": {
        "id": "RbdOvI7Zs04I"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIjsGNMKs01Z",
        "outputId": "d9e8de1b-e78c-4146-be22-f493b608e4e5"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[AIMessage(content='hi!'),\n",
              " HumanMessage(content='what is the capital of france?')]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai_response = chat(history.messages)\n",
        "ai_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2I-MOCJJtQmr",
        "outputId": "d519b25c-23e5-4969-fd2a-d071ead2166f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The capital of France is Paris.', response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 20, 'total_tokens': 27}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None}, id='run-de2fef8f-1ce7-473c-be97-e0d873a83639-0')"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.add_ai_message(ai_response.content)\n",
        "history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_F3vw0L_tR85",
        "outputId": "67620777-34d3-4868-e815-e495f1520015"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[AIMessage(content='hi!'),\n",
              " HumanMessage(content='what is the capital of france?'),\n",
              " AIMessage(content='The capital of France is Paris.')]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Chains** ⛓️⛓️⛓️\n",
        "Combining different LLM calls and action automatically\n",
        "\n",
        "Ex: Summary #1, Summary #2, Summary #3 > Final Summary\n",
        "\n",
        "Check out [this video](https://www.youtube.com/watch?v=f9_BWhCI4Zo&t=2s) explaining different summarization chain types\n",
        "\n",
        "There are [many applications of chains](https://python.langchain.com/en/latest/modules/chains/how_to_guides.html) search to see which are best for your use case.\n",
        "\n",
        "We'll cover two of them:\n",
        "\n",
        "1. Simple Sequential Chains\n",
        "Easy chains where you can use the output of an LLM as an input into another. Good for breaking up tasks (and keeping your LLM focused)"
      ],
      "metadata": {
        "id": "lgxcXpELtWSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import SimpleSequentialChain"
      ],
      "metadata": {
        "id": "N1P_WyeutTXJ"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
        "% USER LOCATION\n",
        "{user_location}\n",
        "\n",
        "YOUR RESPONSE:\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
        "\n",
        "# Holds my 'location' chain\n",
        "location_chain = LLMChain(llm=llm, prompt=prompt_template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ua8wMGVtpAD",
        "outputId": "dfd1abf4-86b4-4c08-b585-dde134178d35"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
        "% MEAL\n",
        "{user_meal}\n",
        "\n",
        "YOUR RESPONSE:\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
        "\n",
        "# Holds my 'meal' chain\n",
        "meal_chain = LLMChain(llm=llm, prompt=prompt_template)"
      ],
      "metadata": {
        "id": "gLjTfM9Htrrd"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)"
      ],
      "metadata": {
        "id": "FYiMviB5t0Ic"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review = overall_chain.run(\"Rome\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fz04Bgit3l1",
        "outputId": "c1de0a00-18a0-4d12-815e-9951442e6bbf"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mCarbonara Pasta\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mIngredients:\n",
            "- 200g spaghetti\n",
            "- 100g pancetta\n",
            "- 2 large eggs\n",
            "- 50g pecorino cheese\n",
            "- Freshly ground black pepper\n",
            "- Salt\n",
            "\n",
            "Instructions:\n",
            "1. Cook spaghetti in a large pot of boiling salted water, until al dente.\n",
            "2. Meanwhile, fry the pancetta in a hot pan until it's crispy.\n",
            "3. In a separate bowl, beat the eggs and mix in about half of the cheese.\n",
            "4. Once the spaghetti is ready, drain it quickly, reserving some of the pasta water.\n",
            "5. Add the drained pasta to the pan with the pancetta. Mix well to coat in the pancetta fat.\n",
            "6. Still in the pan, but away from the heat, add the beaten egg and cheese mixture to the pasta and stir quickly until the eggs thicken and create a creamy sauce. If it's too thick, add some of the reserved pasta water.\n",
            "7. Season with salt and a generous amount of black pepper.\n",
            "8. Serve it immediately, sprinkled with the rest of the cheese.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Summarization Chain\n",
        "Easily run through long numerous documents and get a summary. Check out [this video](https://www.youtube.com/watch?v=f9_BWhCI4Zo) for other chain types besides map-reduce"
      ],
      "metadata": {
        "id": "x_OjYLCMt7D3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = TextLoader('data/PaulGrahamEssays/disc.txt')\n",
        "documents = loader.load()\n",
        "\n",
        "# Get your splitter ready\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
        "\n",
        "# Split your docs inato texts\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# There is a lot of complexity hidden in this one line. I encourage you to check out the video above for more detail\n",
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
        "chain.run(texts)"
      ],
      "metadata": {
        "id": "a5bRfD6Gt5Vm"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Agents 🤖🤖\n",
        "Official LangChain Documentation describes agents perfectly (emphasis mine):\n",
        "\n",
        "    Some applications will require not just a predetermined chain of calls to\n",
        "    LLMs/other tools, but potentially an unknown chain that depends on the\n",
        "    user's input. In these types of chains, there is a “agent” which has access\n",
        "    to a suite of tools. Depending on the user input, the agent can then decide\n",
        "    which, if any, of these tools to call.\n",
        "\n",
        "Basically you use the LLM not just for text output, but also for decision making. The coolness and power of this functionality can't be overstated enough.\n",
        "\n",
        "Sam Altman emphasizes that the LLMs are good '[reasoning engine](https://www.youtube.com/watch?v=L_Guz73e6fw&t=867s)'. Agent take advantage of this.  \n",
        "  \n",
        "Agents  \n",
        "The language model that drives decision making.\n",
        "\n",
        "More specifically, an agent takes in an input and returns a response corresponding to an action to take along with an action input. You can see different types of agents (which are better for different use cases) [here](https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html).  \n",
        "  \n",
        "Tools  \n",
        "A 'capability' of an agent. This is an abstraction on top of a function that makes it easy for LLMs (and agents) to interact with it. Ex: Google search.  \n",
        "  \n",
        "This area shares commonalities with [OpenAI plugins](https://platform.openai.com/docs/plugins/introduction).  \n",
        "  \n",
        "Toolkit  \n",
        "Groups of tools that your agent can select from  \n",
        "  \n",
        "Let's bring them all together:"
      ],
      "metadata": {
        "id": "WnKN-9GduPx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "import json"
      ],
      "metadata": {
        "id": "M3gQnr_6uHTb"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "serpapi_api_key=os.getenv(\"SERP_API_KEY\", \"YourAPIKey\")"
      ],
      "metadata": {
        "id": "vikIyJH1v0Wh"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toolkit = load_tools([\"serpapi\"], llm=llm, serpapi_api_key=serpapi_api_key)"
      ],
      "metadata": {
        "id": "4-s-4rtcvGah"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(toolkit, llm, agent=\"zero-shot-react-description\", verbose=True, return_intermediate_steps=True)"
      ],
      "metadata": {
        "id": "Fdi3YqhovGXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent({\"input\":\"what was the first album of the\"\n",
        "                    \"band that Natalie Bergman is a part of?\"})"
      ],
      "metadata": {
        "id": "c79jDwnavGUz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}