{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQPg9H3O4aiVAXkhqPkrFe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Engineer-D/Projects/blob/main/Understanding_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### https://github.com/JohannesJolkkonen/funktio-ai-samples/tree/main/knowledge-graph-demo"
      ],
      "metadata": {
        "id": "ldSvzPuyF2Fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDxz_hRgGb7-",
        "outputId": "6c419fe1-465c-4335-b74f-4b174a702198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai (from -r requirements.txt (line 1))\n",
            "  Downloading langchain_openai-0.1.6-py3-none-any.whl (34 kB)\n",
            "Collecting openai (from -r requirements.txt (line 2))\n",
            "  Downloading openai-1.25.1-py3-none-any.whl (312 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.9/312.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting graphdatascience (from -r requirements.txt (line 3))\n",
            "  Downloading graphdatascience-1.10-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting retry==0.9.2 (from -r requirements.txt (line 4))\n",
            "  Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting langchain>=0.0.216 (from -r requirements.txt (line 5))\n",
            "  Downloading langchain-0.1.17-py3-none-any.whl (867 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting streamlit==1.23.1 (from -r requirements.txt (line 6))\n",
            "  Downloading streamlit-1.23.1-py2.py3-none-any.whl (8.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting streamlit-chat==0.0.2.2 (from -r requirements.txt (line 7))\n",
            "  Downloading streamlit_chat-0.0.2.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting streamlit-chat-media==0.0.4 (from -r requirements.txt (line 8))\n",
            "  Downloading streamlit_chat_media-0.0.4-py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (2.0.3)\n",
            "Requirement already satisfied: plotly==5.15.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (5.15.0)\n",
            "Requirement already satisfied: altair<5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (4.2.2)\n",
            "Collecting python-dotenv==1.0.0 (from -r requirements.txt (line 13))\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: decorator>=3.4.2 in /usr/local/lib/python3.10/dist-packages (from retry==0.9.2->-r requirements.txt (line 4)) (4.4.2)\n",
            "Collecting py<2.0.0,>=1.4.26 (from retry==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (8.1.7)\n",
            "Collecting importlib-metadata<7,>=1.4 (from streamlit==1.23.1->-r requirements.txt (line 6))\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Collecting packaging<24,>=14.1 (from streamlit==1.23.1->-r requirements.txt (line 6))\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<10,>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (14.0.2)\n",
            "Collecting pympler<2,>=0.9 (from streamlit==1.23.1->-r requirements.txt (line 6))\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.4 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (8.2.3)\n",
            "Requirement already satisfied: toml<2 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (4.11.0)\n",
            "Collecting tzlocal<5,>=1.1 (from streamlit==1.23.1->-r requirements.txt (line 6))\n",
            "  Downloading tzlocal-4.3.1-py3-none-any.whl (20 kB)\n",
            "Collecting validators<1,>=0.2 (from streamlit==1.23.1->-r requirements.txt (line 6))\n",
            "  Downloading validators-0.28.1-py3-none-any.whl (39 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3 (from streamlit==1.23.1->-r requirements.txt (line 6))\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.1.dev5 (from streamlit==1.23.1->-r requirements.txt (line 6))\n",
            "  Downloading pydeck-0.9.0-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.23.1->-r requirements.txt (line 6)) (6.3.3)\n",
            "Collecting watchdog (from streamlit==1.23.1->-r requirements.txt (line 6))\n",
            "  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.46 (from langchain_openai->-r requirements.txt (line 1))\n",
            "  Downloading langchain_core-0.1.50-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken<1,>=0.5.2 (from langchain_openai->-r requirements.txt (line 1))\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai->-r requirements.txt (line 2)) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai->-r requirements.txt (line 2))\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 2)) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 2)) (4.66.2)\n",
            "Collecting multimethod<2.0,>=1.0 (from graphdatascience->-r requirements.txt (line 3))\n",
            "  Downloading multimethod-1.11.2-py3-none-any.whl (10 kB)\n",
            "Collecting neo4j<6.0,>=4.4.2 (from graphdatascience->-r requirements.txt (line 3))\n",
            "  Downloading neo4j-5.20.0.tar.gz (202 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.0/203.0 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting textdistance<5.0,>=4.0 (from graphdatascience->-r requirements.txt (line 3))\n",
            "  Downloading textdistance-4.6.2-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.216->-r requirements.txt (line 5)) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.216->-r requirements.txt (line 5)) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.216->-r requirements.txt (line 5)) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.216->-r requirements.txt (line 5)) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain>=0.0.216->-r requirements.txt (line 5))\n",
            "  Downloading dataclasses_json-0.6.5-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain>=0.0.216->-r requirements.txt (line 5))\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.36 (from langchain>=0.0.216->-r requirements.txt (line 5))\n",
            "  Downloading langchain_community-0.0.36-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain>=0.0.216->-r requirements.txt (line 5))\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain>=0.0.216->-r requirements.txt (line 5))\n",
            "  Downloading langsmith-0.1.53-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 10)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 10)) (2024.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<5->-r requirements.txt (line 12)) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<5->-r requirements.txt (line 12)) (3.1.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<5->-r requirements.txt (line 12)) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<5->-r requirements.txt (line 12)) (0.12.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216->-r requirements.txt (line 5)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216->-r requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216->-r requirements.txt (line 5)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.216->-r requirements.txt (line 5)) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 2)) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 2)) (1.2.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.216->-r requirements.txt (line 5))\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.216->-r requirements.txt (line 5))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3->streamlit==1.23.1->-r requirements.txt (line 6))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 2))\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 2))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit==1.23.1->-r requirements.txt (line 6)) (3.18.1)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain>=0.0.216->-r requirements.txt (line 5))\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<5->-r requirements.txt (line 12)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<5->-r requirements.txt (line 12)) (0.35.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<5->-r requirements.txt (line 12)) (0.18.0)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain>=0.0.216->-r requirements.txt (line 5))\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 2)) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 2)) (2.18.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<5->-r requirements.txt (line 12)) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2->streamlit==1.23.1->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.4->streamlit==1.23.1->-r requirements.txt (line 6)) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.4->streamlit==1.23.1->-r requirements.txt (line 6)) (2.0.7)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.11.0->streamlit==1.23.1->-r requirements.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.11.0->streamlit==1.23.1->-r requirements.txt (line 6)) (2.16.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.0.216->-r requirements.txt (line 5)) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.5.2->langchain_openai->-r requirements.txt (line 1)) (2023.12.25)\n",
            "Collecting pytz-deprecation-shim (from tzlocal<5,>=1.1->streamlit==1.23.1->-r requirements.txt (line 6))\n",
            "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3->streamlit==1.23.1->-r requirements.txt (line 6))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.11.0->streamlit==1.23.1->-r requirements.txt (line 6)) (0.1.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.216->-r requirements.txt (line 5))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: neo4j\n",
            "  Building wheel for neo4j (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neo4j: filename=neo4j-5.20.0-py3-none-any.whl size=280771 sha256=d667c07ad4dd6b63b566caaa48055ab32dfaf5e40b6477c7b14f5b729fe462bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/12/66/764554d079caad4b9a11a02cfc0d200dd876d12935b9cf7e64\n",
            "Successfully built neo4j\n",
            "Installing collected packages: watchdog, validators, textdistance, smmap, pytz-deprecation-shim, python-dotenv, pympler, py, packaging, orjson, neo4j, mypy-extensions, multimethod, jsonpointer, importlib-metadata, h11, tzlocal, typing-inspect, tiktoken, retry, pydeck, marshmallow, jsonpatch, httpcore, gitdb, langsmith, httpx, graphdatascience, gitpython, dataclasses-json, openai, langchain-core, streamlit, langchain-text-splitters, langchain_openai, langchain-community, streamlit-chat-media, streamlit-chat, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 7.1.0\n",
            "    Uninstalling importlib_metadata-7.1.0:\n",
            "      Successfully uninstalled importlib_metadata-7.1.0\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 5.2\n",
            "    Uninstalling tzlocal-5.2:\n",
            "      Successfully uninstalled tzlocal-5.2\n",
            "Successfully installed dataclasses-json-0.6.5 gitdb-4.0.11 gitpython-3.1.43 graphdatascience-1.10 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 importlib-metadata-6.11.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.17 langchain-community-0.0.36 langchain-core-0.1.50 langchain-text-splitters-0.0.1 langchain_openai-0.1.6 langsmith-0.1.53 marshmallow-3.21.2 multimethod-1.11.2 mypy-extensions-1.0.0 neo4j-5.20.0 openai-1.25.1 orjson-3.10.3 packaging-23.2 py-1.11.0 pydeck-0.9.0 pympler-1.0.1 python-dotenv-1.0.0 pytz-deprecation-shim-0.1.0.post0 retry-0.9.2 smmap-5.0.1 streamlit-1.23.1 streamlit-chat-0.0.2.2 streamlit-chat-media-0.0.4 textdistance-4.6.2 tiktoken-0.6.0 typing-inspect-0.9.0 tzlocal-4.3.1 validators-0.28.1 watchdog-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BcIiChIXEwRQ"
      },
      "outputs": [],
      "source": [
        "from langchain.graphs import Neo4jGraph\n",
        "from langchain.chains import GraphCypherQAChain\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dotenv\n",
        "import os"
      ],
      "metadata": {
        "id": "pGjA7wfOGlX8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dotenv.load_dotenv()\n",
        "openai_api_key = os.getenv(\"OPEN_AI_SECRET_KEY\")\n",
        "#Neo4j configuration\n",
        "neo4j_url = os.getenv(\"NEO4J_URI\")\n",
        "neo4j_user = os.getenv(\"NEO4J_USERNAME\")\n",
        "neo4j_password = os.getenv(\"NEO4J_PASSWORD\")"
      ],
      "metadata": {
        "id": "lQvBw6znG6zo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",temperature=0, api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "RN2i3mdmHD2n"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cypher generation prompt\n",
        "cypher_generation_template = \"\"\"\n",
        "You are an expert Neo4j Cypher translator who converts English to Cypher based on the Neo4j Schema provided, following the instructions below:\n",
        "1. Generate Cypher query compatible ONLY for Neo4j Version 5\n",
        "2. Do not use EXISTS, SIZE, HAVING keywords in the cypher. Use alias when using the WITH keyword\n",
        "3. Use only Nodes and relationships mentioned in the schema\n",
        "4. Always do a case-insensitive and fuzzy search for any properties related search. Eg: to search for a Client, use `toLower(client.id) contains 'neo4j'`. To search for Slack Messages, use 'toLower(SlackMessage.text) contains 'neo4j'`. To search for a project, use `toLower(project.summary) contains 'logistics platform' OR toLower(project.name) contains 'logistics platform'`.)\n",
        "5. Never use relationships that are not mentioned in the given schema\n",
        "6. When asked about projects, Match the properties using case-insensitive matching and the OR-operator, E.g, to find a logistics platform -project, use `toLower(project.summary) contains 'logistics platform' OR toLower(project.name) contains 'logistics platform'`.\n",
        "\n",
        "schema: {schema}\n",
        "\n",
        "Examples:\n",
        "Question: Which client's projects use most of our people?\n",
        "Answer: ```MATCH (c:CLIENT)<-[:HAS_CLIENT]-(p:Project)-[:HAS_PEOPLE]->(person:Person)\n",
        "RETURN c.name AS Client, COUNT(DISTINCT person) AS NumberOfPeople\n",
        "ORDER BY NumberOfPeople DESC```\n",
        "Question: Which person uses the largest number of different technologies?\n",
        "Answer: ```MATCH (person:Person)-[:USES_TECH]->(tech:Technology)\n",
        "RETURN person.name AS PersonName, COUNT(DISTINCT tech) AS NumberOfTechnologies\n",
        "ORDER BY NumberOfTechnologies DESC```\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "cypher_prompt = PromptTemplate(\n",
        "    template = cypher_generation_template,\n",
        "    input_variables = [\"schema\", \"question\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "O9RXN0EmHbY4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CYPHER_QA_TEMPLATE = \"\"\"You are an assistant that helps to form nice and human understandable answers.\n",
        "The information part contains the provided information that you must use to construct an answer.\n",
        "The provided information is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\n",
        "Make the answer sound as a response to the question. Do not mention that you based the result on the given information.\n",
        "If the provided information is empty, say that you don't know the answer.\n",
        "Final answer should be easily readable and structured.\n",
        "Information:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "qa_prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"], template=CYPHER_QA_TEMPLATE\n",
        ")"
      ],
      "metadata": {
        "id": "qZGldP2-I7CD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_graph(user_input):\n",
        "    graph = Neo4jGraph(url=neo4j_url, username=neo4j_user, password=neo4j_password)\n",
        "    chain = GraphCypherQAChain.from_llm(\n",
        "        llm=llm,\n",
        "        graph=graph,\n",
        "        verbose=True,\n",
        "        return_intermediate_steps=True,\n",
        "        cypher_prompt=cypher_prompt,\n",
        "        qa_prompt=qa_prompt\n",
        "        )\n",
        "    result = chain(user_input)\n",
        "    return result, chain"
      ],
      "metadata": {
        "id": "FA29npfBI6-f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result, chain = query_graph(\"What's the ultimate goal\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY9vy0HVI68T",
        "outputId": "36484325-f4e4-4282-e82f-2aaaebf35d92"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3mMATCH (g:Goal)\n",
            "RETURN g.description AS UltimateGoal\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UP9bwq3I66L",
        "outputId": "6de3f371-d93b-445c-ba7d-d517baac5e71"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': \"What's the ultimate goal\",\n",
              " 'result': \"I don't know the answer.\",\n",
              " 'intermediate_steps': [{'query': 'MATCH (g:Goal)\\nRETURN g.description AS UltimateGoal'},\n",
              "  {'context': []}]}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.qa_chain.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiS7cg9ZJzS-",
        "outputId": "0c7a0644-f18f-478e-f239-8acc75cf4d56"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an assistant that helps to form nice and human understandable answers.\n",
            "The information part contains the provided information that you must use to construct an answer.\n",
            "The provided information is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\n",
            "Make the answer sound as a response to the question. Do not mention that you based the result on the given information.\n",
            "If the provided information is empty, say that you don't know the answer.\n",
            "Final answer should be easily readable and structured.\n",
            "Information:\n",
            "{context}\n",
            "\n",
            "Question: {question}\n",
            "Helpful Answer:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SC test"
      ],
      "metadata": {
        "id": "Xhg1d6LbK9LG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGFmBMRVbxxX",
        "outputId": "7ab97966-7f86-4f3d-e4e8-920313255a0c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "openAIEmbeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "\n",
        "#Are firewall and router configuration standards established and implemented? Please pick the appropriate answer\n",
        "query = \"what is the Firewall Type?\"\n",
        "\n",
        "configurations = [\n",
        "    \"Firewall Configuration:\",\n",
        "    \"-----------------------\",\n",
        "    \"Firewall Type: Cisco ASA\",\n",
        "    \"Security Level: High\",\n",
        "    \"Access Control Lists (ACLs):\",\n",
        "    \"    - ACL 101: Permit TCP any host 192.168.1.1 eq www\",\n",
        "    \"    - ACL 102: Deny IP any any\",\n",
        "    \"\",\n",
        "    \"Router Configuration:\",\n",
        "    \"---------------------\",\n",
        "    \"Router Type: Cisco ISR 4000\",\n",
        "    \"Routing Protocol: OSPF\",\n",
        "    \"Interfaces:\",\n",
        "    \"    - GigabitEthernet0/0: 192.168.1.1/24\",\n",
        "    \"    - GigabitEthernet0/1: 10.0.0.1/24\",\n",
        "    \"    - Serial0/0/0: 172.16.1.1/30\",\n",
        "    \"\",\n",
        "    \"VPN Configuration:\",\n",
        "    \"------------------\",\n",
        "    \"VPN Type: Site-to-Site IPsec VPN\",\n",
        "    \"Tunnel Mode: Tunnel Mode\",\n",
        "    \"Encryption Algorithm: AES-256\",\n",
        "    \"Authentication Method: Pre-shared Key\",\n",
        "    \"Peer IP Address: 203.0.113.2\",\n",
        "    \"Tunnel Interface: Tunnel0\",\n",
        "    \"    - IP Address: 192.168.100.1/30\",\n",
        "    \"    - Peer IP Address: 203.0.113.2\",\n",
        "]\n",
        "\n",
        "# Prompt\n",
        "template = \"\"\"As a compliance assistant you are to help users answer questions presented to them. where possible, reply with just Yes or No\n",
        "example:\n",
        "context: \"Firewall, routers and configuration have been implemented in our network security\"\n",
        "Quesion: Has necessary configuration been put in place?\n",
        "Answer: Yes\n",
        "\n",
        "Instruction: Don't add '.' after Yes, Also keep your answers brief\n",
        "\n",
        "Context: {context}\n",
        "using the context above answer the question {question}\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "QA_CHAIN_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=template,\n",
        "    stream = False,\n",
        ")\n",
        "\n",
        "vectorstore = Chroma.from_texts(texts=configurations, embedding=openAIEmbeddings)\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",temperature=0, api_key=openai_api_key)\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=vectorstore.as_retriever(), #retriever=vectorstore.as_retriever()\n",
        "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
        ")\n",
        "\n",
        "result = qa_chain({\"query\": query})\n",
        "print(result['result'])"
      ],
      "metadata": {
        "id": "1N0EpoSxJ2G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(qa_chain.combine_documents_chain.llm_chain.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhRPqOWhbsdE",
        "outputId": "636f472d-cf79-4822-c14c-f7916f7aa72c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a compliance assistant you are to help users answer questions presented to them. where possible, reply with just Yes or No\n",
            "example:\n",
            "context: \"Firewall, routers and configuration have been implemented in our network security\"\n",
            "Quesion: Has necessary configuration been put in place?\n",
            "Answer: Yes\n",
            "\n",
            "Instruction: Don't add '.' after Yes, Also keep your answers brief\n",
            "\n",
            "Context: {context}\n",
            "using the context above answer the question {question}\n",
            "Helpful Answer:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain.combine_documents_chain.llm_chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2smf2-HcMlR",
        "outputId": "c174ec30-e0d8-457b-ac89-04d1d9efc57c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template='As a compliance assistant you are to help users answer questions presented to them. where possible, reply with just Yes or No\\nexample:\\ncontext: \"Firewall, routers and configuration have been implemented in our network security\"\\nQuesion: Has necessary configuration been put in place?\\nAnswer: Yes\\n\\nInstruction: Don\\'t add \\'.\\' after Yes, Also keep your answers brief\\n\\nContext: {context}\\nusing the context above answer the question {question}\\nHelpful Answer:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7e33df4a12a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7e33df465ff0>, model_name='gpt-3.5-turbo-0125', temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy=''))"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6S_NaXzwcykd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### https://www.youtube.com/watch?v=J_0qvRt4LNk&list=PL8motc6AQftk1Bs42EW45kwYbyJ4jOdiZ"
      ],
      "metadata": {
        "id": "-kCesKD-GQwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install openai langchain langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vr9G1X-DGQcX",
        "outputId": "0e7671be-13b8-4873-8cbc-3c784cfd5230"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/1.8 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('OPENAI_KEY')"
      ],
      "metadata": {
        "id": "jcg7n1uQGel2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "63sjoO8AGQWw"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo',\n",
        "             temperature=0.9,\n",
        "             max_tokens = 256,\n",
        "            api_key = api_key)"
      ],
      "metadata": {
        "id": "_fLAV6dHGQUp"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate"
      ],
      "metadata": {
        "id": "OLJeCzwfGQSU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restaurant_template = \"\"\"\n",
        "I want you to act as a naming consultant for new restaurants.\n",
        "\n",
        "Return a list of restaurant names. Each name should be short, catchy and easy to remember. It shoud relate to the type of restaurant you are naming.\n",
        "\n",
        "What are some good names for a restaurant that is {restaurant_desription}?\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"restaurant_desription\"],\n",
        "    template=restaurant_template,\n",
        ")"
      ],
      "metadata": {
        "id": "RgHE4j4fHBHs"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "description = \"a Greek place that serves fresh lamb souvlakis and other Greek food \"\n",
        "description_02 = \"a burger place that is themed with baseball memorabilia\"\n",
        "description_03 = \"a cafe that has live hard rock music and memorabilia\"\n",
        "\n",
        "## to see what the prompt will be like\n",
        "print(prompt_template.format(restaurant_desription=description))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhk7pUpxHHGC",
        "outputId": "74632e3f-abb6-4cb6-9ff4-49be48c5eaca"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "I want you to act as a naming consultant for new restaurants.\n",
            "\n",
            "Return a list of restaurant names. Each name should be short, catchy and easy to remember. It shoud relate to the type of restaurant you are naming.\n",
            "\n",
            "What are some good names for a restaurant that is a Greek place that serves fresh lamb souvlakis and other Greek food ?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## querying the model with the prompt template\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "\n",
        "chain = LLMChain(llm=llm,\n",
        "                 prompt=prompt_template)\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "print(chain.run(description))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W_gYokrHLcT",
        "outputId": "d96dfb0b-29fa-4647-e1c0-22b2410695c5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Lamb & Olive\n",
            "2. Greek Grill House\n",
            "3. Souvlaki Spot\n",
            "4. Gyro Haven\n",
            "5. Acropolis Eats\n",
            "6. Zeus' Bites\n",
            "7. Olive Tree Kitchen\n",
            "8. Mykonos Grille\n",
            "9. The Greek Table\n",
            "10. Hellenic Flavors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the chain only specifying the input variable.\n",
        "print(chain.run({\"restaurant_desription\":description_02}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlZ2oTsSHWW-",
        "outputId": "59470bf1-287f-4cf1-ec88-b86af78db7c5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Home Run Burgers\n",
            "2. Grand Slam Grill\n",
            "3. The Ballpark Burger Co.\n",
            "4. Bat & Burger Bistro\n",
            "5. Diamond Diner\n",
            "6. Burger Base\n",
            "7. Triple Play Burgers\n",
            "8. Slugger's Burgers & Fries\n",
            "9. The All-Star Burger Joint\n",
            "10. Batter Up Bites\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ihfWFCmxIpOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### https://github.com/gkamradt/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%201%20-%20Fundamentals.ipynb"
      ],
      "metadata": {
        "id": "5eIuz9FfeRaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### LangChain Cookbook 👨‍🍳👩‍🍳\n",
        "\n",
        "*This cookbook is based off the [LangChain Conceptual Documentation](https://docs.langchain.com/docs/)*\n",
        "\n",
        "**Goal:** Provide an introductory understanding of the components and use cases of LangChain via [ELI5](https://www.dictionary.com/e/slang/eli5/#:~:text=ELI5%20is%20short%20for%20%E2%80%9CExplain,a%20complicated%20question%20or%20problem.) examples and code snippets. For use cases check out [part 2](https://github.com/gkamradt/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%202%20-%20Use%20Cases.ipynb). See [video tutorial](https://www.youtube.com/watch?v=2xxziIWmaSA) of this notebook.\n",
        "\n",
        " **Links:**  \n",
        "* [LC Conceptual Documentation](https://docs.langchain.com/docs/)\n",
        "* [LC Python Documentation](https://python.langchain.com/en/latest/)\n",
        "* [LC Javascript/Typescript Documentation](https://js.langchain.com/docs/)\n",
        "* [LC Discord](https://discord.gg/6adMQxSpJS)\n",
        "* [www.langchain.com](https://langchain.com/)\n",
        "* [LC Twitter](https://twitter.com/LangChainAI)  \n",
        "\n",
        "### **What is LangChain?**\"\n",
        "> LangChain is a framework for developing applications powered by language models.\n",
        "    \n",
        "**~~TL~~DR**: LangChain makes the complicated parts of working & building with AI models easier. It helps do this in two ways:\n",
        "\n",
        "1. **Integration** - Bring external data, such as your files, other applications, and api data, to your LLMs\\n\",\n",
        "2. **Agency** - Allow your LLMs to interact with it's environment via decision making. Use LLMs to help decide which action to take next  \n",
        "\n",
        "### **Why LangChain?**\n",
        "1. **Components** - LangChain makes it easy to swap out abstractions and components necessary to work with language models.\n",
        "2. **Customized Chains** - LangChain provides out of the box support for using and customizing 'chains' - a series of actions strung together.\n",
        "3. **Speed 🚢** - This team ships insanely fast. You'll be up to date with the latest LLM features.\n",
        "4. **Community 👥** - Wonderful discord and community support, meet ups, hackathons, etc.\n",
        "Though LLMs can be straightforward (text-in, text-out) you'll quickly run into friction points that LangChain helps with once you develop more complicated applications.\n",
        "\n",
        "*Note: This cookbook will not cover all aspects of LangChain. It's contents have been curated to get you to building & impact as quick as possible. For more, please check out [LangChain Conceptual Documentation](https://docs.langchain.com/docs/)*\n",
        "\n",
        "*Update Oct '23: This notebook has been expanded from it's original form*\n",
        "\n",
        "You'll need an OpenAI api key to follow this tutorial. You can have it as an environement variable, in an .env file where this jupyter notebook lives, or insert it below where 'YourAPIKey' is. Have if you have questions on this, put these instructions into [ChatGPT](https://chat.openai.com/)."
      ],
      "metadata": {
        "id": "knNhtz_ueltK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langchain langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA49-fYVeV7E",
        "outputId": "ad376423-1bde-45c3-f748-faa3b02e3bc2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.7/116.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('OPENAI_KEY')"
      ],
      "metadata": {
        "id": "EHKS_Xp-gefZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### LangChain Components  \n",
        "\n",
        "## Schema - Nuts and Bolts of working with Large Language Models (LLMs)  \n",
        "\n",
        "## Text  \n",
        "The natural language way to interact with LLMs"
      ],
      "metadata": {
        "id": "hBJ_5VtbglCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You'll be working with simple strings (that'll soon grow in complexity!)\n",
        "my_text = \"What day comes after Friday?\"\n",
        "my_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Wn1a8wUWeV3q",
        "outputId": "6fcfd226-fccf-412a-e634-cf1050990a40"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What day comes after Friday?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Chat Messages\n",
        "Like text, but specified with a message type (System, Human, AI)\n",
        "\n",
        "* System - Helpful background context that tell the AI what to do\n",
        "* Human - Messages that are intented to represent the user\n",
        "* AI - Messages that show what the AI responded with  \n",
        "\n",
        "For more, see OpenAI's [documentation](https://platform.openai.com/docs/guides/chat/introduction)"
      ],
      "metadata": {
        "id": "6tl7GCaXhmmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "# This it the language model we'll use. We'll talk about what we're doing below in the next section\n",
        "chat = ChatOpenAI(temperature=.7, openai_api_key=api_key)"
      ],
      "metadata": {
        "id": "_cnw_CLEeV1D"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create a few messages that simulate a chat experience with a bot"
      ],
      "metadata": {
        "id": "dp0Ga88ph-2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out what to eat in one short sentence\"),\n",
        "        HumanMessage(content=\"I like tomatoes, what should I eat?\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrQDQGNmeVyY",
        "outputId": "8009dd87-2d00-4978-e7a8-12d4e232f16e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='You might enjoy a tomato, basil, and mozzarella salad.', response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 39, 'total_tokens': 52}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_a450710239', 'finish_reason': 'stop', 'logprobs': None}, id='run-f1d21361-630f-4aa2-af52-52348183a8ec-0')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also pass more chat history w/ responses from the AI"
      ],
      "metadata": {
        "id": "vcVlDhAyiWMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out where to travel in one short sentence\"),\n",
        "        HumanMessage(content=\"I like the beaches where should I go?\"),\n",
        "        AIMessage(content=\"You should go to Nice, France\"),\n",
        "        HumanMessage(content=\"What else should I do when I'm there?\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c42w9kreVv9",
        "outputId": "02911220-4a2f-46cb-cd0d-f769e7be1a9c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Explore the charming Old Town and enjoy the vibrant street markets in Nice, France.', response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 64, 'total_tokens': 80}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_a450710239', 'finish_reason': 'stop', 'logprobs': None}, id='run-662a715a-4b7a-4ffc-ba00-cf6e3d285b53-0')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also exclude the system message if you want"
      ],
      "metadata": {
        "id": "xGsupzHcib8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\n",
        "    [\n",
        "        HumanMessage(content=\"What day comes after Thursday?\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hyND1DJeVtp",
        "outputId": "414b030e-8775-45b2-9ff9-30c657f00131"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Friday', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 13, 'total_tokens': 14}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None}, id='run-6c876534-e6b2-46ad-923f-5da4893343b7-0')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Documents**  \n",
        "An object that holds a piece of text and metadata (more information about that text)"
      ],
      "metadata": {
        "id": "OgHenqHYiePn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document"
      ],
      "metadata": {
        "id": "KGxylaS1eVrO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\",\n",
        "         metadata={\n",
        "             'my_document_id' : 234234,\n",
        "             'my_document_source' : \"The LangChain Papers\",\n",
        "             'my_document_create_time' : 1680013019\n",
        "         })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZkD_FngeVpC",
        "outputId": "9b00a849-d287-4ee5-9b7b-7838af1933b0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\", metadata={'my_document_id': 234234, 'my_document_source': 'The LangChain Papers', 'my_document_create_time': 1680013019})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But you don't have to include metadata if you don't want to"
      ],
      "metadata": {
        "id": "1HoXTJu3jIEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfpEHAiAeVmy",
        "outputId": "3a6ea12c-bb34-4557-94c2-1561576c58f2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mrj33a5IjLRp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}